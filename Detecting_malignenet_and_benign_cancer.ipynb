{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Here in this code, i have a dataset that contain data about many features of tumors, and i will be making a machine learning pipeline for predicting if cancer is malginent or benign, to help doctors in their tests"
      ],
      "metadata": {
        "id": "0m4TcXNCWDmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First i will upload the libraries we will need to upload the dataset and split the data and test and train the data"
      ],
      "metadata": {
        "id": "dWiNEAnFWjfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3njl1ynm46V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "then i will upload the dataset\n",
        "\n",
        "knowing that there are many columns, i will use 'display.max_columns' option to display all of them and look for any non numeric or categorial data, if any non numeric exist i must encode it"
      ],
      "metadata": {
        "id": "2CKccBlmWxMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df = pd.read_csv(\"Cancer_Data.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "v-W0VTArpEa1",
        "outputId": "ebdd121f-f0f2-4f79-e852-14971cb2d983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0      842302         M        17.99         10.38          122.80     1001.0   \n",
              "1      842517         M        20.57         17.77          132.90     1326.0   \n",
              "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3    84348301         M        11.42         20.38           77.58      386.1   \n",
              "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
              "..        ...       ...          ...           ...             ...        ...   \n",
              "564    926424         M        21.56         22.39          142.00     1479.0   \n",
              "565    926682         M        20.13         28.25          131.20     1261.0   \n",
              "566    926954         M        16.60         28.08          108.30      858.1   \n",
              "567    927241         M        20.60         29.33          140.10     1265.0   \n",
              "568     92751         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     symmetry_mean  fractal_dimension_mean  radius_se  texture_se  \\\n",
              "0           0.2419                 0.07871     1.0950      0.9053   \n",
              "1           0.1812                 0.05667     0.5435      0.7339   \n",
              "2           0.2069                 0.05999     0.7456      0.7869   \n",
              "3           0.2597                 0.09744     0.4956      1.1560   \n",
              "4           0.1809                 0.05883     0.7572      0.7813   \n",
              "..             ...                     ...        ...         ...   \n",
              "564         0.1726                 0.05623     1.1760      1.2560   \n",
              "565         0.1752                 0.05533     0.7655      2.4630   \n",
              "566         0.1590                 0.05648     0.4564      1.0750   \n",
              "567         0.2397                 0.07016     0.7260      1.5950   \n",
              "568         0.1587                 0.05884     0.3857      1.4280   \n",
              "\n",
              "     perimeter_se  area_se  smoothness_se  compactness_se  concavity_se  \\\n",
              "0           8.589   153.40       0.006399         0.04904       0.05373   \n",
              "1           3.398    74.08       0.005225         0.01308       0.01860   \n",
              "2           4.585    94.03       0.006150         0.04006       0.03832   \n",
              "3           3.445    27.23       0.009110         0.07458       0.05661   \n",
              "4           5.438    94.44       0.011490         0.02461       0.05688   \n",
              "..            ...      ...            ...             ...           ...   \n",
              "564         7.673   158.70       0.010300         0.02891       0.05198   \n",
              "565         5.203    99.04       0.005769         0.02423       0.03950   \n",
              "566         3.425    48.55       0.005903         0.03731       0.04730   \n",
              "567         5.772    86.22       0.006522         0.06158       0.07117   \n",
              "568         2.548    19.15       0.007189         0.00466       0.00000   \n",
              "\n",
              "     concave points_se  symmetry_se  fractal_dimension_se  radius_worst  \\\n",
              "0              0.01587      0.03003              0.006193        25.380   \n",
              "1              0.01340      0.01389              0.003532        24.990   \n",
              "2              0.02058      0.02250              0.004571        23.570   \n",
              "3              0.01867      0.05963              0.009208        14.910   \n",
              "4              0.01885      0.01756              0.005115        22.540   \n",
              "..                 ...          ...                   ...           ...   \n",
              "564            0.02454      0.01114              0.004239        25.450   \n",
              "565            0.01678      0.01898              0.002498        23.690   \n",
              "566            0.01557      0.01318              0.003892        18.980   \n",
              "567            0.01664      0.02324              0.006185        25.740   \n",
              "568            0.00000      0.02676              0.002783         9.456   \n",
              "\n",
              "     texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0            17.33           184.60      2019.0           0.16220   \n",
              "1            23.41           158.80      1956.0           0.12380   \n",
              "2            25.53           152.50      1709.0           0.14440   \n",
              "3            26.50            98.87       567.7           0.20980   \n",
              "4            16.67           152.20      1575.0           0.13740   \n",
              "..             ...              ...         ...               ...   \n",
              "564          26.40           166.10      2027.0           0.14100   \n",
              "565          38.25           155.00      1731.0           0.11660   \n",
              "566          34.12           126.70      1124.0           0.11390   \n",
              "567          39.42           184.60      1821.0           0.16500   \n",
              "568          30.37            59.16       268.6           0.08996   \n",
              "\n",
              "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0              0.66560           0.7119                0.2654          0.4601   \n",
              "1              0.18660           0.2416                0.1860          0.2750   \n",
              "2              0.42450           0.4504                0.2430          0.3613   \n",
              "3              0.86630           0.6869                0.2575          0.6638   \n",
              "4              0.20500           0.4000                0.1625          0.2364   \n",
              "..                 ...              ...                   ...             ...   \n",
              "564            0.21130           0.4107                0.2216          0.2060   \n",
              "565            0.19220           0.3215                0.1628          0.2572   \n",
              "566            0.30940           0.3403                0.1418          0.2218   \n",
              "567            0.86810           0.9387                0.2650          0.4087   \n",
              "568            0.06444           0.0000                0.0000          0.2871   \n",
              "\n",
              "     fractal_dimension_worst  Unnamed: 32  \n",
              "0                    0.11890          NaN  \n",
              "1                    0.08902          NaN  \n",
              "2                    0.08758          NaN  \n",
              "3                    0.17300          NaN  \n",
              "4                    0.07678          NaN  \n",
              "..                       ...          ...  \n",
              "564                  0.07115          NaN  \n",
              "565                  0.06637          NaN  \n",
              "566                  0.07820          NaN  \n",
              "567                  0.12400          NaN  \n",
              "568                  0.07039          NaN  \n",
              "\n",
              "[569 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd0c5a75-56f6-4e43-98c7-0d67c7f284d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd0c5a75-56f6-4e43-98c7-0d67c7f284d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd0c5a75-56f6-4e43-98c7-0d67c7f284d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd0c5a75-56f6-4e43-98c7-0d67c7f284d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 461
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the values i saw for the last column (Unnamed: 32) are all NaN, so i will check if all of them are like this, if their is similar to the number of rows then i will delete the column"
      ],
      "metadata": {
        "id": "GB71Hc3eYIyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnull = df['Unnamed: 32'].isna().sum()\n",
        "dfnull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZHuRel61diF",
        "outputId": "9d76c571-855f-425d-bf1b-474086e9da1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {},
          "execution_count": 462
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "569 is equal to the number of rows, so i will delete the column and also the id column because it's not needed in the training and don't offer any information."
      ],
      "metadata": {
        "id": "Kwgtu6S5ZD2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['id', 'Unnamed: 32'], axis=1)"
      ],
      "metadata": {
        "id": "PNiACgGTsL0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i want to check if all the values in the diagnosis column are malignent or benign (no corrupted data in the column), i will count how many times both values are mentioned and then see it's equal to the number of total rows"
      ],
      "metadata": {
        "id": "WA2jwwfwaFUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C1 = (df[\"diagnosis\"] == 'M').sum()\n",
        "C2 = (df[\"diagnosis\"] == 'B').sum()\n",
        "print(C1)\n",
        "print(C2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTXsNy9dp6sG",
        "outputId": "46f17dda-f7b8-4601-f34b-aaee44eeb95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "212\n",
            "357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "then i will turn them to 1 and 0, so that they could be used for the trainig and testing of models"
      ],
      "metadata": {
        "id": "5ZkOFzAVb3aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'] = df['diagnosis'] .replace({'B': 1, 'M': 0})"
      ],
      "metadata": {
        "id": "1kWZ0nh6ps-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i will split the dataset to train and test sets, and since the data isn't very large, i will take less data for test than 20%"
      ],
      "metadata": {
        "id": "mWL9ndfHdBPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.15)\n",
        "print(\"df:\", df.shape)\n",
        "print(\"df_train:\", df_train.shape)\n",
        "print(\"df_test:\", df_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCNv4zfzuIgs",
        "outputId": "f0a440fa-d6d6-4b1f-c7f2-faa4fc900f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df: (569, 31)\n",
            "df_train: (483, 31)\n",
            "df_test: (86, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "kK4aLoCddrST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First i will check for empty values to remove them in the training set"
      ],
      "metadata": {
        "id": "3KhOsYwT9mxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnull = df_train.isna().sum()\n",
        "dfnull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JMtQXtZ9qwx",
        "outputId": "e782b609-fbe3-4b9c-e283-6c9d46e7b617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "diagnosis                  0\n",
              "radius_mean                0\n",
              "texture_mean               0\n",
              "perimeter_mean             0\n",
              "area_mean                  0\n",
              "smoothness_mean            0\n",
              "compactness_mean           0\n",
              "concavity_mean             0\n",
              "concave points_mean        0\n",
              "symmetry_mean              0\n",
              "fractal_dimension_mean     0\n",
              "radius_se                  0\n",
              "texture_se                 0\n",
              "perimeter_se               0\n",
              "area_se                    0\n",
              "smoothness_se              0\n",
              "compactness_se             0\n",
              "concavity_se               0\n",
              "concave points_se          0\n",
              "symmetry_se                0\n",
              "fractal_dimension_se       0\n",
              "radius_worst               0\n",
              "texture_worst              0\n",
              "perimeter_worst            0\n",
              "area_worst                 0\n",
              "smoothness_worst           0\n",
              "compactness_worst          0\n",
              "concavity_worst            0\n",
              "concave points_worst       0\n",
              "symmetry_worst             0\n",
              "fractal_dimension_worst    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 467
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "then i will check for duplicate rows, and i will delete if any exists"
      ],
      "metadata": {
        "id": "0v90UKhyduh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfduplicate =df_train.duplicated().sum()\n",
        "dfduplicate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jen1O4Ow2BZe",
        "outputId": "e6f15417-33df-4a4c-99eb-f9eb8e09991d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 468
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i will check for outliers to delete, the dataset isn't very big so i will try to delete less number of the data\n",
        "\n",
        "but here i will only pick the columns with big differece between their min and max values.\n",
        "\n",
        "first i will check the max and min values in them, then remove the outliers"
      ],
      "metadata": {
        "id": "1FeHtrThd6tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minval = df_train.min()\n",
        "maxval = df_train.max()\n",
        "print(minval)\n",
        "print(maxval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHctzOvG0zQD",
        "outputId": "0688fd97-a16c-40dd-a942-3c87ddc88e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diagnosis                    0.000000\n",
            "radius_mean                  6.981000\n",
            "texture_mean                 9.710000\n",
            "perimeter_mean              43.790000\n",
            "area_mean                  143.500000\n",
            "smoothness_mean              0.052630\n",
            "compactness_mean             0.019380\n",
            "concavity_mean               0.000000\n",
            "concave points_mean          0.000000\n",
            "symmetry_mean                0.106000\n",
            "fractal_dimension_mean       0.049960\n",
            "radius_se                    0.111500\n",
            "texture_se                   0.360200\n",
            "perimeter_se                 0.757000\n",
            "area_se                      6.802000\n",
            "smoothness_se                0.001713\n",
            "compactness_se               0.002252\n",
            "concavity_se                 0.000000\n",
            "concave points_se            0.000000\n",
            "symmetry_se                  0.007882\n",
            "fractal_dimension_se         0.000950\n",
            "radius_worst                 7.930000\n",
            "texture_worst               12.020000\n",
            "perimeter_worst             50.410000\n",
            "area_worst                 185.200000\n",
            "smoothness_worst             0.071170\n",
            "compactness_worst            0.027290\n",
            "concavity_worst              0.000000\n",
            "concave points_worst         0.000000\n",
            "symmetry_worst               0.156500\n",
            "fractal_dimension_worst      0.055040\n",
            "dtype: float64\n",
            "diagnosis                     1.00000\n",
            "radius_mean                  28.11000\n",
            "texture_mean                 39.28000\n",
            "perimeter_mean              188.50000\n",
            "area_mean                  2501.00000\n",
            "smoothness_mean               0.16340\n",
            "compactness_mean              0.34540\n",
            "concavity_mean                0.42680\n",
            "concave points_mean           0.20120\n",
            "symmetry_mean                 0.30400\n",
            "fractal_dimension_mean        0.09575\n",
            "radius_se                     2.87300\n",
            "texture_se                    4.88500\n",
            "perimeter_se                 21.98000\n",
            "area_se                     542.20000\n",
            "smoothness_se                 0.03113\n",
            "compactness_se                0.13540\n",
            "concavity_se                  0.39600\n",
            "concave points_se             0.05279\n",
            "symmetry_se                   0.07895\n",
            "fractal_dimension_se          0.02984\n",
            "radius_worst                 36.04000\n",
            "texture_worst                49.54000\n",
            "perimeter_worst             251.20000\n",
            "area_worst                 4254.00000\n",
            "smoothness_worst              0.20060\n",
            "compactness_worst             1.05800\n",
            "concavity_worst               1.25200\n",
            "concave points_worst          0.29100\n",
            "symmetry_worst                0.57740\n",
            "fractal_dimension_worst       0.20750\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saw that the columns that have biggest differences between their min amd max values are [area_mean,area_se,area_worst], so i will only remove the outlier values in them"
      ],
      "metadata": {
        "id": "I96_uIHcgcRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df_train['area_mean'].quantile(0.10)\n",
        "Q3 = df_train['area_mean'].quantile(0.90)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df_train = df_train[(df_train['area_mean'] >= Q1 - 1.5 * IQR) & (df_train['area_mean'] <= Q3 + 1.5 * IQR)]"
      ],
      "metadata": {
        "id": "oAa70h_l2KKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df_train['area_se'].quantile(0.10)\n",
        "Q3 = df_train['area_se'].quantile(0.90)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df_train = df_train[(df_train['area_se'] >= Q1 - 1.5 * IQR) & (df_train['area_se'] <= Q3 + 1.5 * IQR)]"
      ],
      "metadata": {
        "id": "8UsHGhgM_LP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df_train['area_worst'].quantile(0.10)\n",
        "Q3 = df_train['area_worst'].quantile(0.90)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df_train = df_train[(df_train['area_worst'] >= Q1 - 1.5 * IQR) & (df_train['area_worst'] <= Q3 + 1.5 * IQR)]"
      ],
      "metadata": {
        "id": "q9tv0pkSBUtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENhcsuVj81ws",
        "outputId": "341aec75-ec8b-46ec-8fb7-4002369fa6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(479, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and here i will split the train and test data for a sets for the the target label, and a sets for the other feautres"
      ],
      "metadata": {
        "id": "6tpPKCnlitOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = df_train.drop([\"diagnosis\"], axis = 1)\n",
        "y_train = df_train[\"diagnosis\"]\n",
        "\n",
        "x_test = df_test.drop([\"diagnosis\"], axis = 1)\n",
        "y_test = df_test[\"diagnosis\"]\n",
        "\n",
        "print(\"x train size:\", x_train.shape)\n",
        "print(\"x test size:\", x_test.shape)\n",
        "print(\"y train size:\", y_train.shape)\n",
        "print(\"y test size:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oTPAOoe9Rn8",
        "outputId": "6d1d9bfd-23a1-4b8c-eef2-d94ddf6d4699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x train size: (479, 30)\n",
            "x test size: (86, 30)\n",
            "y train size: (479,)\n",
            "y test size: (86,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, Evaluate and test"
      ],
      "metadata": {
        "id": "vNy1LrhWjLf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i will try to find the best model to train it and then test it\n",
        "i must upload the libriaries i need for the training and testing of the model\n",
        "I will use several ensemble learning methods and then pick the best using gridsearch\n",
        "\n",
        "first i will upload the libraries needed for the\n",
        "and ensemble learning methods"
      ],
      "metadata": {
        "id": "Lm4d3loxjPFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "jh96JDPb9xpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the ensemble learning algorithms i picked are random forest for it's accuracy and handling complex relations between features, Extra trees because it's faster and the dataset have many dimensions, ADA because it's effective for binary classification, Voting classifier to use severla methods at once"
      ],
      "metadata": {
        "id": "DwJQy4ifX03M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "IaTpQkzIn2Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will tune the hyperparameters for the Randomforestclassifer, i will set criterion for different ways to test impurity of splits, i will pick the default and a lower value for the n_estimator (since our dataset isn't very large)"
      ],
      "metadata": {
        "id": "goz5WU7cBs0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFC_parameters_grid = {\"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
        "                   \"n_estimators\": ( 50, 100),\n",
        "                        \"max_depth\": (5,10,None)}\n",
        "RFC_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(),\n",
        "             RFC_parameters_grid, scoring=\"accuracy\")\n",
        "RFC_model.fit(x_train, y_train)\n",
        "print (\"accuracy {:.2f}\".format(RFC_model.best_score_))\n",
        "print (\"best found hyperparameters  = {}\".format(RFC_model.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhdcB_NB6wW8",
        "outputId": "b21d51bf-30eb-4729-c788-a3a4ac86db8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.96\n",
            "best found hyperparameters  = {'criterion': 'log_loss', 'max_depth': 10, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for ada boosting classifier i will also pick some lower values with the default ones for learning rate and n_estimators, because the dataset isn't large, also different algoritms"
      ],
      "metadata": {
        "id": "td16OUD9JL3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_parameters_grid = { \"learning_rate\" : [ 0.1, 0.2, 1.0],\n",
        "                   \"n_estimators\": (20, 50),\n",
        "                   \"algorithm\": ('SAMME', 'SAMME.R')}\n",
        "ADAmodel = sklearn.model_selection.GridSearchCV(sklearn.ensemble.AdaBoostClassifier(),\n",
        "             ada_parameters_grid, scoring=\"accuracy\")\n",
        "ADAmodel.fit(x_train, y_train)\n",
        "print (\"accuracy {:.2f}\".format(ADAmodel.best_score_))\n",
        "print (\"best found hyperparameters = {}\".format(ADAmodel.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5GawSBl9nKm",
        "outputId": "b6b8b071-4fd4-419d-adbe-04391fcdc76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.96\n",
            "best found hyperparameters = {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in Extra tree classifier, i also use default/lower values for n_estimators and max depth, several criterion methods,and raise minumum decrease of impurity needed to split a node and allow out of bag samples"
      ],
      "metadata": {
        "id": "QCE-AWeJVwO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ETC_parameters_grid = { \"n_estimators\": [50,100],\n",
        "                       \"criterion\": ['gini', 'entropy'],\n",
        "                        \"max_depth\": [None,5,10],\n",
        "                       \"min_impurity_decrease\":[0.0, 0.5],\n",
        "                   \"oob_score\": (True, False)}\n",
        "ETCmodel = sklearn.model_selection.GridSearchCV(sklearn.ensemble.ExtraTreesClassifier(),\n",
        "             ETC_parameters_grid, scoring=\"accuracy\")\n",
        "ETCmodel.fit(x_train, y_train)\n",
        "print (\"accuracy {:.2f}\".format(ETCmodel.best_score_))\n",
        "print (\"best found hyperparameters = {}\".format(ETCmodel.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I9Pdgf1A18U",
        "outputId": "6da14c8d-50ca-4bdc-c8cb-5afe9a5a6675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.97\n",
            "best found hyperparameters = {'criterion': 'gini', 'max_depth': None, 'min_impurity_decrease': 0.0, 'n_estimators': 50, 'oob_score': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "120 fits failed out of a total of 240.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "120 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
            "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.96870614        nan 0.96241228        nan 0.63256579\n",
            "        nan 0.63256579        nan 0.95201754        nan 0.94572368\n",
            "        nan 0.63256579        nan 0.63256579        nan 0.96245614\n",
            "        nan 0.96241228        nan 0.63256579        nan 0.63256579\n",
            "        nan 0.96662281        nan 0.96451754        nan 0.63256579\n",
            "        nan 0.63256579        nan 0.94991228        nan 0.94780702\n",
            "        nan 0.63256579        nan 0.63256579        nan 0.96035088\n",
            "        nan 0.96662281        nan 0.63256579        nan 0.63256579]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the voting classifier, i must first upload and use some classificaiton algorithms and tune them\n",
        "\n",
        "For svm/svc, i will try 3 different kernels\n",
        "\n",
        "for logistic regression i will try using all processors and or just one\n",
        "\n",
        "for Multinomial naiv Bayes, i will use prior probabilities or not\n",
        "\n",
        "for the decision tree, to measure quality of the tree spliting i will try gini and entropy\n",
        "different tree depths, and numbers required for split to avoid overfitting\n",
        "put some values for min_impurity_decrease to put more restriction for splits in the tree by putting minimum amount of impurity decrease to split a node\n",
        "\n",
        "for the Voting Classifier i will use both hard and soft voting"
      ],
      "metadata": {
        "id": "-kQvf-CnbQ_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "T = DecisionTreeClassifier()\n",
        "S = SVC()\n",
        "MNB = MultinomialNB()\n",
        "L = LogisticRegression()\n",
        "\n",
        "T_param_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, 10],\n",
        "                'min_samples_split': [1,2],'min_impurity_decrease': [1,2,0.0]}\n",
        "S_param_grid = {'kernel': ['linear', 'rbf', 'poly']}\n",
        "L_param_grid = { 'n_jobs': [None, -1]}\n",
        "MNB_param_grid = {'fit_prior': [True,False]}"
      ],
      "metadata": {
        "id": "7gCkjs039pCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimations = [('dt', T), ('svm',S), ('NB',MNB), ('LR',L)]\n",
        "VC_parameters_grid = { \"voting\": ['hard', 'Soft']}\n",
        "VCmodel = sklearn.model_selection.GridSearchCV(sklearn.ensemble.VotingClassifier(estimations),\n",
        "             VC_parameters_grid, scoring=\"accuracy\")\n",
        "VCmodel.fit(x_train, y_train)\n",
        "print (\"accuracy {:.2f}\".format(VCmodel.best_score_))\n",
        "print (\"best found hyperparameters = {}\".format(VCmodel.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nks3Ti1AEc4T",
        "outputId": "40c5e166-5204-4408-83dc-603a90e1baca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.95\n",
            "best found hyperparameters = {'voting': 'hard'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "5 fits failed out of a total of 10.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\", line 335, in fit\n",
            "    self._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 600, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'voting' parameter of VotingClassifier must be a str among {'hard', 'soft'}. Got 'Soft' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.94570175        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and here i want to pick the best model"
      ],
      "metadata": {
        "id": "POcQ6gbVdP_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bestmodel =  RFC_model.best_estimator_ if RFC_model.best_score_ > ADAmodel.best_score_ and RFC_model.best_score_ > ETCmodel.best_score_ and RFC_model.best_score_ > VCmodel.best_score_ else ADAmodel.best_estimator_ if ADAmodel.best_score_ > ETCmodel.best_score_ and ADAmodel.best_score_ > VCmodel.best_score_ else ETCmodel.best_estimator_ if ETCmodel.best_score_ > VCmodel.best_score_ else VCmodel.best_estimator_\n",
        "Bestmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "u6BNAnFQpeFM",
        "outputId": "5c9c301c-1e64-4955-8395-4893f738ecc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(n_estimators=50)"
            ],
            "text/html": [
              "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier(n_estimators=50)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and here i will see the result of the model on the test set"
      ],
      "metadata": {
        "id": "xuqFvKJ1dYs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Bestmodel.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"accuracy:\", accuracy)\n",
        "print(\"confusion matrix:\\n\", CM)\n",
        "print(\"FI:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4o09oRmz3bA",
        "outputId": "83f15de1-5d2e-4e02-cb35-3b321f3f3fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9534883720930233\n",
            "confusion matrix:\n",
            " [[28  4]\n",
            " [ 0 54]]\n",
            "FI: 0.9642857142857143\n",
            "Precision: 0.9310344827586207\n",
            "recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discussion:\n",
        "\n",
        "\n",
        "We got some good results at the end of this pipeline, with few notes:\n",
        "\n",
        "\n",
        "The accuracy number is very well, when we see the recall number, it suggests that the model is doing perfect with positive instnaces (predicting Malignent tumor) and labeled as such in the dataset, precision shows that almost all positives were corectly predicted, F1 score show us that the model have great performance becasue it's close to 1.\n",
        "\n",
        "\n",
        "as for the confusion matrix, the model also seems to be almost perfect, and it was able to predict all positive instances correctly\n",
        "\n",
        "\n",
        "but we have to take into account that there is a gap (but not big) between the count of positive and negative instances, with Benign being more, we can conclude that there maybe still need for some experimentation by adding or removing data by appling data augmentation for malignent instances,or dimensionality reduction on Benign instnaces.\n",
        "\n",
        "\n",
        "for later improvement, we can consider adding or removing more data, and also having different sizes for the test set, however considering the dataset was small, traditional predicting models was possibly sufficent"
      ],
      "metadata": {
        "id": "iceJMyCcdxTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# other experiments\n",
        "\n",
        "\n",
        "_I also tried to use Perceptron (the code is at the end of this page) and we have a good result of around 0.85, but it seems testing many models was with ensemble and gridsearchCV gave use more options\n",
        "\n",
        "_ i also replaced the quantile values for Q1 and Q3 of 0.10 and 0.90 with o.15 and 0.85, and i got an accuracy result of 0.94 with other metrics being also high but little bit less, which most likely mean it was better to keep as much data as possible"
      ],
      "metadata": {
        "id": "vu9VanZZq0bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset refrencing:\n",
        "\n",
        "Taha, E. (2023) Cancer data, Kaggle. Available at: https://www.kaggle.com/datasets/erdemtaha/cancer-data (Accessed: 27 June 2023)."
      ],
      "metadata": {
        "id": "0jAhbOIaSaGn"
      }
    }
  ]
}